{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f2175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "import cv2\n",
    "import math\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import torch as pt\n",
    "from torchvision import transforms as trans\n",
    "from torch import nn\n",
    "from imutils.video import FileVideoStream\n",
    "from statistics import mode\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5806c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### timer\n",
    "\n",
    "t1 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ecacff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the video\n",
    "v_cap = cv2.VideoCapture('Data/videoplayback.mp4')\n",
    "v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = int(v_cap.get(cv2.CAP_PROP_FPS))\n",
    "timestamps = [] #list for the timestamps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d7553",
   "metadata": {},
   "source": [
    "### Face detection part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34dc48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(select_largest=False, device='cuda', keep_all=False,post_process=False, image_size=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e9f33f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282387\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753f533b21f947f4a69f4b871cedc98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/282387 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTCNN batch num0 from:0 to:[1980]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raphael\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n",
      "C:\\Users\\Raphael\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:183: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
      "C:\\Users\\Raphael\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:339: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  boxes = np.array(boxes)\n",
      "C:\\Users\\Raphael\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:340: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  probs = np.array(probs)\n",
      "C:\\Users\\Raphael\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:341: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  points = np.array(points)\n",
      "C:\\Users\\Raphael\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:444: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  selected_boxes = np.array(selected_boxes)\n",
      "C:\\Users\\Raphael\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:446: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  selected_points = np.array(selected_points)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTCNN batch num1 from:1981 to:[3962]\n",
      "MTCNN batch num2 from:3963 to:[5944]\n",
      "MTCNN batch num3 from:5945 to:[7925]\n",
      "MTCNN batch num4 from:7926 to:[9421]\n"
     ]
    }
   ],
   "source": [
    "# Loop through video\n",
    "batch_size = 2048\n",
    "batch_num = 0\n",
    "frames = []\n",
    "boxes = []\n",
    "landmarks = []\n",
    "view_frames = []\n",
    "view_boxes = []\n",
    "view_landmarks = []\n",
    "print(v_len)\n",
    "sf = 1 # get one frame per second\n",
    "lff = False # last frame fail flag\n",
    "\n",
    "for i in tqdm(range(v_len)):\n",
    "    # Load frame\n",
    "    success, frame = v_cap.read()\n",
    "    if i % math.floor(fps)*sf == 0 or i >= v_len-1: #skip to get every sf seconds and get the last frame for processing\n",
    "        success, frame = v_cap.retrieve()\n",
    "    else:\n",
    "        continue\n",
    "    if not success and i < v_len-1:\n",
    "        continue\n",
    "    else:\n",
    "        if i >= v_len-1: #in case is the last frame and it failed\n",
    "            lff = True\n",
    "\n",
    "    if not lff:\n",
    "        # Add to batch, resizing for speed\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frame = frame.resize([int(f * 0.25) for f in frame.size])\n",
    "        frames.append(frame)\n",
    "        timestamps.append(math.floor(v_cap.get(cv2.CAP_PROP_POS_MSEC))//1000) #append timestamp of the frame\n",
    "    \n",
    "    # When batch is full, detect faces and reset batch list\n",
    "    if len(frames) >= batch_size or i >= v_len-1:\n",
    "        print(\"MTCNN batch num\" + str(batch_num) + \" from:\" + str(timestamps[0]) + \" to:\" + str(timestamps[-1:]))\n",
    "        #define save path \n",
    "        save_paths = [f'Data/T4/{timestamps[i]}.jpg' for i in range(len(frames))]\n",
    "        faces = mtcnn(frames, save_path=save_paths)\n",
    "        im_count = 0\n",
    "        batch_num += 1\n",
    "        frames = [] #clear frame list\n",
    "        timestamps = [] #clear timestamplist\n",
    "        \n",
    "    outi = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24cbb231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195.838458776474\n"
     ]
    }
   ],
   "source": [
    "t2 = time.time()\n",
    "delta1 = t2-t1\n",
    "\n",
    "print(delta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52698af9",
   "metadata": {},
   "source": [
    "### Emotion detection part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49044e5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raphael\\AppData\\Local\\Temp\\ipykernel_7932\\3496881956.py:15: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  np_images=np.array(images)\n",
      "C:\\Users\\Raphael\\AppData\\Local\\Temp\\ipykernel_7932\\3496881956.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np_images=np.array(images)\n"
     ]
    }
   ],
   "source": [
    "#0 - happy\n",
    "#1 - sad\n",
    "#2 - neutral\n",
    "images = []\n",
    "time_stamps=[]\n",
    "for i in range(10): #to deal with OSError: [Errno 24] Too many open files: 'Data/T4\\\\8484.jpg'\n",
    "    filelist = glob.glob(f'Data/T4/{i}*')\n",
    "    for f in filelist:\n",
    "        time_stamp=int(os.path.split(f)[1].split('.')[0])\n",
    "        time_stamps.append(time_stamp)\n",
    "\n",
    "    imagestmp = [Image.open(f).copy() for f in filelist]\n",
    "    images = images+imagestmp\n",
    "np_time_stamps=np.array(time_stamps)\n",
    "np_images=np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0550b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDetector(nn.Module):\n",
    "    def __init__(self, in_ch,classes):\n",
    "        super(EmotionDetector,self).__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels=in_ch,out_channels=60, kernel_size=(3,3))\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.bn1=nn.BatchNorm2d(60)\n",
    "        self.maxPool1=nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "\n",
    "\n",
    "        self.conv2=nn.Conv2d(in_channels=60,out_channels=120, kernel_size=(3,3))\n",
    "        self.relu2=nn.ReLU()\n",
    "        self.bn2=nn.BatchNorm2d(120)\n",
    "        self.maxPool2=nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "\n",
    "\n",
    "        self.conv3=nn.Conv2d(in_channels=120,out_channels=240, kernel_size=(3,3))\n",
    "        self.relu3=nn.ReLU()\n",
    "        self.bn3=nn.BatchNorm2d(240)\n",
    "        # self.maxPool3=nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        #28*28*1 -> 27-13-> 12-6 *80 -> 5*4*100\n",
    "        #    \n",
    "        self.dropout=nn.Dropout2d()\n",
    "\n",
    "        self.conv4=nn.Conv2d(in_channels=240,out_channels=480, kernel_size=(3,3))\n",
    "        self.relu4=nn.ReLU()\n",
    "        self.maxPool4=nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "\n",
    "\n",
    "\n",
    "        # self.conv5=nn.Conv2d(in_channels=120,out_channels=150, kernel_size=(5,5))\n",
    "        # self.relu5=nn.ReLU()\n",
    "        # self.maxPool5=nn.MaxPool2d(kernel_size=(2,2), stride=(1,1))\n",
    "\n",
    "\n",
    "        self.fc1=nn.Linear(in_features=480*3*3,out_features=200)\n",
    "        self.relu_fc1=nn.ReLU()\n",
    "\n",
    "        self.fc2=nn.Linear(in_features=200,out_features=classes)\n",
    "        self.lsm=nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        debug=False\n",
    "        x=x.float()\n",
    "        if debug : print(\"input to cv1 \",x.shape)\n",
    "        x=self.conv1(x)\n",
    "        if debug : print(\"input to mxp1\",x.shape)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu1(x)\n",
    "        x=self.maxPool1(x)\n",
    "        # x=self.dropout(x)\n",
    "\n",
    "\n",
    "        if debug :print(\"input to cv2\",x.shape)\n",
    "        x=self.conv2(x)\n",
    "        x=self.bn2(x)\n",
    "\n",
    "        if debug : print(\"input to mxp2\",x.shape)\n",
    "        x=self.relu2(x)\n",
    "        x=self.maxPool2(x)\n",
    "        # x=self.dropout(x)\n",
    "\n",
    "        if debug : print(\"input to cv3\",x.shape)\n",
    "        x=self.conv3(x)\n",
    "        x=self.bn3(x)\n",
    "        x=self.relu3(x)\n",
    "        if debug : print(\"input to mxp3\",x.shape)\n",
    "        # x=self.maxPool3(x)\n",
    "        x=self.dropout(x)\n",
    "\n",
    "\n",
    "        x=self.conv4(x)\n",
    "        x=self.relu4(x)\n",
    "        x=self.maxPool4(x)\n",
    "        x=self.dropout(x)\n",
    "        # x=self.conv5(x)\n",
    "        # x=self.relu5(x)\n",
    "        # x=self.maxPool5(x)\n",
    "\n",
    "        if debug : print(\"input to fc1\",x.shape)\n",
    "        x = pt.flatten(x,1)\n",
    "\n",
    "        x=self.fc1(x)\n",
    "        x=self.relu_fc1(x)\n",
    "        if debug : print(\"input to fc2 \",x.shape)\n",
    "        x=self.fc2(x)\n",
    "        if debug : print(\"input to lsm \",x.shape)\n",
    "        output=self.lsm(x)\n",
    "        if debug : print(\"final output \",x.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f348e24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME= 'EMOTION_DETECTOR_1.pt'\n",
    "MODEL_PATH = f\"models//{MODEL_NAME}\" \n",
    "CLASSES=3\n",
    "model=EmotionDetector(in_ch=1,classes=CLASSES).to('cuda')\n",
    "model.load_state_dict(pt.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d94557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9287\n"
     ]
    }
   ],
   "source": [
    "print(len(np_images))\n",
    "output_emo=[]\n",
    "model.eval()\n",
    "with pt.no_grad():\n",
    "    for image, times in zip(np_images,np_time_stamps):\n",
    "      image2 = Image.fromarray(np.uint8(image)).convert('L')\n",
    "      img = trans.ToTensor()(image2).type(pt.FloatTensor).to(\"cuda\").reshape(1,1,48,48)\n",
    "      pred=pt.exp(model(img))\n",
    "      idx=pred.argmax(axis=1).cpu().numpy()[0]\n",
    "      output_emo.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ce236b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 1, 2, 2]\n",
      "[999]\n"
     ]
    }
   ],
   "source": [
    "print(output_emo[1:50])\n",
    "print(time_stamps[-1:])\n",
    "np_emo=np.array(output_emo)\n",
    "np_ts=np.array(time_stamps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38be356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort lists\n",
    "\n",
    "sEmotion = Z = [x for _,x in sorted(zip(time_stamps,output_emo))]\n",
    "sTime = time_stamps\n",
    "sTime.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e08df7",
   "metadata": {},
   "source": [
    "### Data Viz part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095c769",
   "metadata": {},
   "source": [
    "#set graph shape\n",
    "sns.set(rc={'figure.figsize':(20,2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583c498",
   "metadata": {},
   "source": [
    "i =0\n",
    "#every 1 min get mode\n",
    "step =60\n",
    "#new time stamps every element represent a minute\n",
    "ticks=[]\n",
    "emotion_min=[]\n",
    "min=0\n",
    "while i +step < len(np_emo):\n",
    "  #dont panic, jsut calculating the mode,\n",
    "  #returns a tuple of lists get first e from tuple then first from list\n",
    "  emotion_min.append(stats.mode(np_emo[i:i+step])[0][0])\n",
    "  ticks.append(min)\n",
    "  min+=1\n",
    "  i=i+step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d747c9a8",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame()\n",
    "df['emotion']=emotion_min\n",
    "df['time_Stamps']=ticks\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e76daf",
   "metadata": {},
   "source": [
    "#sns.set(rc={'figure.figsize':(16,5)})\n",
    "sns.histplot(data=df, x='time_Stamps', hue='emotion', multiple='fill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258773f",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123e2f1",
   "metadata": {},
   "source": [
    "df=pd.DataFrame()\n",
    "df['emotion']=output_emo\n",
    "df['time_Stamps']=time_stamps\n",
    "df.head()\n",
    "\n",
    "df.to_csv('emotions.csv')\n",
    "\n",
    "#sns.set(rc={'figure.figsize':(16,5)})\n",
    "sns.histplot(data=df, x='time_Stamps', hue='emotion', multiple='fill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcaef56",
   "metadata": {},
   "source": [
    "#### Viz with mode 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb27ce",
   "metadata": {},
   "source": [
    "def percentageEvaluationMode(value_list, timestamp_list, sliceSize):\n",
    "    outputList = []\n",
    "    timestamps = []\n",
    "    partitionSize = math.floor(len(value_list)/sliceSize)\n",
    "    #print(partitionSize)\n",
    "\n",
    "    for i in range(sliceSize):\n",
    "        #print(partitionSize*i)\n",
    "        #print(partitionSize*(i+1))\n",
    "        outputList.append(mode(value_list[partitionSize*i:partitionSize*(i+1)]))\n",
    "        timestamps.append(timestamp_list[partitionSize*i])\n",
    "    return outputList, timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbae22d",
   "metadata": {},
   "source": [
    "normalizedEmotions, normalizedTimes = percentageEvaluationMode(output_emo, time_stamps, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88c713",
   "metadata": {},
   "source": [
    "df2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65c8a3",
   "metadata": {},
   "source": [
    "df2['emotion']=normalizedEmotions\n",
    "df2['time_Stamps']=normalizedTimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0085e4",
   "metadata": {},
   "source": [
    "sns.histplot(data=df2, x='time_Stamps', hue='emotion', multiple='fill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c65fc9",
   "metadata": {},
   "source": [
    "#### Sum Viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ab095",
   "metadata": {},
   "source": [
    "def percentageEvaluationSum(value_list, timestamp_list, sliceSize):\n",
    "    outputListHappy = []\n",
    "    outputListSad = []\n",
    "    timestamps = []\n",
    "    partitionSize = math.floor(len(value_list)/sliceSize)\n",
    "    print(partitionSize)\n",
    "    for i in range(sliceSize):\n",
    "        happy_cnt = 0\n",
    "        sad_cnt = 0\n",
    "        for j in range(partitionSize*i, partitionSize*(i+1)):\n",
    "            if value_list[j] == 0:\n",
    "                happy_cnt += 1\n",
    "            if value_list[j] == 1:\n",
    "                sad_cnt += 1\n",
    "        \n",
    "        outputListHappy.append(happy_cnt)\n",
    "        outputListSad.append(sad_cnt)\n",
    "        \n",
    "        timestamps.append(timestamp_list[partitionSize*i])\n",
    "    return outputListHappy, outputListSad , timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228a0e44",
   "metadata": {},
   "source": [
    "normHappy, normSad, normalizedTimes = percentageEvaluationSum(output_emo, time_stamps, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19493a0f",
   "metadata": {},
   "source": [
    "df3 = pd.DataFrame()\n",
    "df3['happy'] = normHappy\n",
    "df3['sad'] = normSad\n",
    "df3['Time'] = normalizedTimes\n",
    "\n",
    "df3.set_index('Time')\n",
    "\n",
    "df3 =df3.melt('Time', var_name='emotion', value_name='value')\n",
    "\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc825b85",
   "metadata": {},
   "source": [
    "sns.set(rc={'figure.figsize':(20,2)})\n",
    "sns.lineplot(data=df3, x='Time', y='value', hue='emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddaac8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e0cefdc",
   "metadata": {},
   "source": [
    "#### Overall emotioness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942829f",
   "metadata": {},
   "source": [
    "normHappy, normSad, normalizedTimes = percentageEvaluationSum(output_emo, time_stamps, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab358d66",
   "metadata": {},
   "source": [
    "df4 = pd.DataFrame()\n",
    "df4['emotioness'] = [x + y for x, y in zip(normHappy, normSad)]\n",
    "df4['time'] = normalizedTimes\n",
    "\n",
    "df4 = df4.set_index('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e21f08",
   "metadata": {},
   "source": [
    "sns.set(rc={'figure.figsize':(20,2)})\n",
    "sns.lineplot(data=df4, x='time', y='emotioness')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2fab5",
   "metadata": {},
   "source": [
    "sns.set(rc={'figure.figsize':(20,2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4104609",
   "metadata": {},
   "source": [
    "type(normHappy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84462c67",
   "metadata": {},
   "source": [
    "df4 =  df4.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65c4db",
   "metadata": {},
   "source": [
    "ax = sns.heatmap(df4, center=0, cmap='mako')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fcfba8",
   "metadata": {},
   "source": [
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596f11e",
   "metadata": {},
   "source": [
    "#### Average emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164cd05b",
   "metadata": {},
   "source": [
    "def percentageEvaluationAvg(value_list, timestamp_list, sliceSize):\n",
    "    outputList = []\n",
    "    timestamps = []\n",
    "    partitionSize = math.floor(len(value_list)/sliceSize)\n",
    "    print(partitionSize)\n",
    "    for i in range(sliceSize):\n",
    "        emotion_cnt = 0\n",
    "        for j in range(partitionSize*i, partitionSize*(i+1)):\n",
    "            if value_list[j] == 0:\n",
    "                emotion_cnt += 1\n",
    "            if value_list[j] == 1:\n",
    "                emotion_cnt -= 1\n",
    "        \n",
    "        outputList.append(emotion_cnt/partitionSize) \n",
    "        timestamps.append(timestamp_list[partitionSize*i])\n",
    "    return outputList , timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0cf059",
   "metadata": {},
   "source": [
    "avgEmotion, avgTime = percentageEvaluationAvg(output_emo, time_stamps, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b457874",
   "metadata": {},
   "source": [
    "df5 = pd.DataFrame()\n",
    "df5['emotion'] = avgEmotion\n",
    "df5['time'] = avgTime\n",
    "\n",
    "df5 = df5.set_index('time')\n",
    "\n",
    "df5 =  df5.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a739ba",
   "metadata": {},
   "source": [
    "ax = sns.heatmap(df5, center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88caed4f",
   "metadata": {},
   "source": [
    "df5 =  df5.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186987af",
   "metadata": {},
   "source": [
    "sns.set(rc={'figure.figsize':(20,2)})\n",
    "sns.lineplot(data=df5, x='time', y='emotion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f239c1",
   "metadata": {},
   "source": [
    "#### Averaged Simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269880cc",
   "metadata": {},
   "source": [
    "def percentageEvaluationAvgS(value_list, timestamp_list, sliceSize):\n",
    "    outputList = []\n",
    "    timestamps = []\n",
    "    partitionSize = math.floor(len(value_list)/sliceSize)\n",
    "    print(partitionSize)\n",
    "    for i in range(sliceSize):\n",
    "        emotion_cnt = 0\n",
    "        for j in range(partitionSize*i, partitionSize*(i+1)):\n",
    "            if value_list[j] == 0:\n",
    "                emotion_cnt += 1\n",
    "            if value_list[j] == 1:\n",
    "                emotion_cnt -= 1\n",
    "        if emotion_cnt/partitionSize > 0.5:\n",
    "            outputList.append(1)\n",
    "        else: \n",
    "            if emotion_cnt/partitionSize < -0.5:\n",
    "                outputList.append(-1)\n",
    "            else:\n",
    "                outputList.append(0)\n",
    "        timestamps.append(timestamp_list[partitionSize*i]//60)\n",
    "    return outputList , timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc8019f",
   "metadata": {},
   "source": [
    "avgEmotionS, avgTimeS = percentageEvaluationAvgS(sEmotion, sTime, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349314a",
   "metadata": {},
   "source": [
    "avgTimeS[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a4ef3",
   "metadata": {},
   "source": [
    "df6 = pd.DataFrame()\n",
    "df6['emotion'] = avgEmotionS\n",
    "df6['time'] = avgTimeS\n",
    "\n",
    "df6 = df6.set_index('time')\n",
    "\n",
    "df6 =  df6.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f9b81",
   "metadata": {},
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a4dd7",
   "metadata": {},
   "source": [
    "sns.set(rc={'figure.figsize':(20,1)})\n",
    "ax = sns.heatmap(df6, center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe384b24",
   "metadata": {},
   "source": [
    "df6 =  df6.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc546caf",
   "metadata": {},
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5a909",
   "metadata": {},
   "source": [
    "sns.set(rc={'figure.figsize':(20,2)})\n",
    "sns.lineplot(data=df6, x='time', y='emotion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732b183",
   "metadata": {},
   "source": [
    "#### General visualization class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07e0cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 - happy\n",
    "#1 - sad\n",
    "#2 - neutral\n",
    "\n",
    "\n",
    "class visualizeEmotion:\n",
    "    def __init__(self, emotion_list, timestamps):\n",
    "        self.emotion_list = emotion_list\n",
    "        self.timestamps = timestamps\n",
    "    \n",
    "    def percentageEvaluationMode(self, value_list, timestamp_list, sliceSize):\n",
    "        outputList = []\n",
    "        timestamps = []\n",
    "        partitionSize = math.floor(len(value_list)/sliceSize)\n",
    "        #print(partitionSize)\n",
    "\n",
    "        for i in range(sliceSize):\n",
    "            #print(partitionSize*i)\n",
    "            #print(partitionSize*(i+1))\n",
    "            outputList.append(mode(value_list[partitionSize*i:partitionSize*(i+1)]))\n",
    "            timestamps.append(timestamp_list[partitionSize*i])\n",
    "        return outputList, timestamps\n",
    "    \n",
    "    def percentageEvaluationAvgT(self, value_list, timestamp_list, sliceSize, th):\n",
    "        outputList = []\n",
    "        timestamps = []\n",
    "        partitionSize = math.floor(len(value_list)/sliceSize)\n",
    "        print(partitionSize)\n",
    "        for i in range(sliceSize):\n",
    "            emotion_cnt = 0\n",
    "            for j in range(partitionSize*i, partitionSize*(i+1)):\n",
    "                if value_list[j] == 0:\n",
    "                    emotion_cnt += 1\n",
    "                if value_list[j] == 1:\n",
    "                    emotion_cnt -= 1\n",
    "            if emotion_cnt/partitionSize > th:\n",
    "                outputList.append(1)\n",
    "            else: \n",
    "                if emotion_cnt/partitionSize < th*-1:\n",
    "                    outputList.append(-1)\n",
    "                else:\n",
    "                    outputList.append(0)\n",
    "            timestamps.append(timestamp_list[partitionSize*i]//60)\n",
    "        return outputList , timestamps\n",
    "    \n",
    "    \n",
    "    def ModeView(self, sliceSize=100, type = 'hist'):\n",
    "        normalizedEmotions, normalizedTimes = self.percentageEvaluationMode(self.emotion_list, self.timestamps, sliceSize)\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        df['emotion']=normalizedEmotions\n",
    "        df['time_Stamps']=normalizedTimes\n",
    "        \n",
    "        sns.set(rc={'figure.figsize':(20,2)})\n",
    "        \n",
    "        if type == 'hist':\n",
    "            sns.histplot(data=df, x='time_Stamps', hue='emotion', multiple='fill')\n",
    "        else:\n",
    "            if type == 'line':\n",
    "                df['emotion'].replace(1,-1) #replaces sad with -1\n",
    "                df['emotion'].replace(0, 1) #replaces happy with 1\n",
    "                df['emotion'].replace(2, 0) #replaces neutral with 1\n",
    "                \n",
    "                sns.lineplot(data=df, x='time_Stamps', y='emotion')\n",
    "            else:\n",
    "                raise ValueError('Supported types are: \"hist\" and \"line\"')\n",
    "    \n",
    "    def averageThresholdView(self, sliceSize=100, threshold=0.3):\n",
    "        avgEmotionS, avgTimeS = self.percentageEvaluationAvgT(self.emotion_list, self.timestamps, sliceSize, threshold)\n",
    "        df = pd.DataFrame()\n",
    "        df['emotion'] = avgEmotionS\n",
    "        df['time'] = avgTimeS\n",
    "\n",
    "        df = df.set_index('time')\n",
    "\n",
    "        df =  df.transpose()\n",
    "        \n",
    "        sns.set(rc={'figure.figsize':(20,1)})\n",
    "        colors = [\"#489FB5\", \"#EDE7E3\", \"#FFA62B\"]\n",
    "        ax = sns.heatmap(df, center=0, cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors))\n",
    "        \n",
    "        df =  df.transpose()\n",
    "        #sns.lineplot(data=df, x='time', y='emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25af7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VE = visualizeEmotion(sEmotion, sTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef98488",
   "metadata": {},
   "source": [
    "VE.ModeView(20, type='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bb16422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x72 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VE.averageThresholdView(100, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "116d5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### timer\n",
    "\n",
    "t3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f726f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212.51931643486023\n"
     ]
    }
   ],
   "source": [
    "delta2= t3-t1\n",
    "print(delta2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
